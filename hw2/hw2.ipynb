{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ML - HW2-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYlaRwNu7ojq"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emUd7uS7crTz"
      },
      "source": [
        "## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
        "\n",
        "This homework is a multiclass classification task, \n",
        "we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
        "\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVUGfWTo7_Oj"
      },
      "source": [
        "## Download Data\n",
        "Download data from google drive, then unzip it.\n",
        "\n",
        "You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n",
        "`timit_11/`\n",
        "- `train_11.npy`: training data<br>\n",
        "- `train_label_11.npy`: training label<br>\n",
        "- `test_11.npy`:  testing data<br><br>\n",
        "\n",
        "**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzkiMEcC3Foq",
        "outputId": "b00bccdc-ddf8-485f-8739-16d03ef2c8af"
      },
      "source": [
        "!gdown --id '1D49K2N_775YL2VZxF24nW25w6Z8G8Ekv' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1D49K2N_775YL2VZxF24nW25w6Z8G8Ekv\n",
            "To: /content/data.zip\n",
            "372MB [00:06, 59.8MB/s]\n",
            "Archive:  data.zip\n",
            "   creating: timit_11/\n",
            "  inflating: timit_11/train_11.npy   \n",
            "  inflating: timit_11/test_11.npy    \n",
            "  inflating: timit_11/train_label_11.npy  \n",
            "data.zip  sample_data  timit_11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_4anls8Drv"
      },
      "source": [
        "## Preparing Data\n",
        "Load the training and testing data from the `.npy` file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJjLT8em-y9G",
        "outputId": "34c174b7-e58b-4652-db93-5bad94dccd2e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Loading data ...')\n",
        "\n",
        "data_root='./timit_11/'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "train = np.reshape(train, (train.shape[0], 11, 39))\n",
        "test = np.reshape(test, (test.shape[0], 11, 39))\n",
        "\n",
        "print('Size of training data: {}'.format(train.shape))\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 11, 39)\n",
            "Size of testing data: (451552, 11, 39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us5XW_x6udZQ"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjf5EcmJtf4e"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        \n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otIC6WhGeh9v"
      },
      "source": [
        "Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYqi_lAuvC59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3647e79d-3b3a-49b7-a0e0-8cbcea58aae3"
      },
      "source": [
        "VAL_RATIO = 0.1\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
        "print('Size of training set: {}'.format(train_x.shape))\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: (1106938, 11, 39)\n",
            "Size of validation set: (122994, 11, 39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbCfclUIgMTX"
      },
      "source": [
        "Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUCbQvqJurYc"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train_x, train_y)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SY7X0lUgb50"
      },
      "source": [
        "Cleanup the unneeded variables to save memory.<br>\n",
        "\n",
        "**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8rzkGraeYeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "904c0f36-bbbf-44f6-e376-4fc2bcd7be1d"
      },
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, train_x, train_y, val_x, val_y\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "238"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqKNvNZwe3V"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYr1ng5fh9pA"
      },
      "source": [
        "Define model architecture, you are encouraged to change and experiment with the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbZrwT6Ny0XL"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layer1 = nn.Conv1d(39, 1100, 3)\n",
        "        self.layer2 = nn.Conv1d(1100, 1000, 1)\n",
        "        self.layer3 = nn.Linear(3000, 2048)\n",
        "        self.layer4 = nn.Linear(2048, 1024)\n",
        "        self.layer5 = nn.Linear(1024, 256)\n",
        "        self.out = nn.Linear(256, 39) \n",
        "\n",
        "        self.layer = nn.MaxPool1d(3)       \n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(1100)\n",
        "        self.bn2 = nn.BatchNorm1d(1000)\n",
        "        self.bn3 = nn.BatchNorm1d(2048)\n",
        "        self.bn4 = nn.BatchNorm1d(1024)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "        self.bn6 = nn.BatchNorm1d(39)\n",
        "\n",
        "        self.act_fn = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.layer1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.layer(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # print(x.shape)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.layer5(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        x = self.bn6(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRYciXZvPbYh"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y114Vmm3Ja6o"
      },
      "source": [
        "#check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEX-yjHjhGuH"
      },
      "source": [
        "Fix random seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88xPiUnm0tAd"
      },
      "source": [
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbBcBXkSp6RA"
      },
      "source": [
        "Feel free to change the training parameters here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTp3ZXg1yO9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a3c26b9-9ddf-4de3-e12c-a2f175463b29"
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "same_seeds(0)\n",
        "\n",
        "# get device \n",
        "device = get_device()\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# training parameters\n",
        "num_epoch = 200                    # number of training epoch\n",
        "learning_rate = 0.0001      # learning rate\n",
        "\n",
        "# the path where checkpoint saved\n",
        "model_path = './model.ckpt'\n",
        "\n",
        "# create model, define a loss function, and optimizer\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.92, 0.999), eps=1e-08)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdMWsBs7zzNs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "646e0bd4-56c8-41bb-bf42-76c6e915498c"
      },
      "source": [
        "# start training\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(inputs) \n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        batch_loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # validation\n",
        "    if len(val_set) > 0:\n",
        "        model.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                batch_loss = criterion(outputs, labels) \n",
        "                _, val_pred = torch.max(outputs, 1) \n",
        "            \n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "\n",
        "            # if the model improves, save a checkpoint at this epoch\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "    else:\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "# if not validating, save the last epoch\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('saving model at last epoch')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/200] Train Acc: 0.394219 Loss: 2.374298 | Val Acc: 0.579679 loss: 1.627815\n",
            "saving model with acc 0.580\n",
            "[002/200] Train Acc: 0.552296 Loss: 1.785917 | Val Acc: 0.642901 loss: 1.351855\n",
            "saving model with acc 0.643\n",
            "[003/200] Train Acc: 0.598111 Loss: 1.565930 | Val Acc: 0.674862 loss: 1.202037\n",
            "saving model with acc 0.675\n",
            "[004/200] Train Acc: 0.623457 Loss: 1.431260 | Val Acc: 0.691700 loss: 1.110299\n",
            "saving model with acc 0.692\n",
            "[005/200] Train Acc: 0.639909 Loss: 1.336704 | Val Acc: 0.704449 loss: 1.040872\n",
            "saving model with acc 0.704\n",
            "[006/200] Train Acc: 0.652074 Loss: 1.265384 | Val Acc: 0.713010 loss: 0.990930\n",
            "saving model with acc 0.713\n",
            "[007/200] Train Acc: 0.660645 Loss: 1.210271 | Val Acc: 0.718100 loss: 0.953928\n",
            "saving model with acc 0.718\n",
            "[008/200] Train Acc: 0.668635 Loss: 1.163944 | Val Acc: 0.721938 loss: 0.924472\n",
            "saving model with acc 0.722\n",
            "[009/200] Train Acc: 0.674592 Loss: 1.127827 | Val Acc: 0.725905 loss: 0.897532\n",
            "saving model with acc 0.726\n",
            "[010/200] Train Acc: 0.680083 Loss: 1.096076 | Val Acc: 0.729800 loss: 0.874509\n",
            "saving model with acc 0.730\n",
            "[011/200] Train Acc: 0.685201 Loss: 1.069232 | Val Acc: 0.732109 loss: 0.856142\n",
            "saving model with acc 0.732\n",
            "[012/200] Train Acc: 0.689077 Loss: 1.046840 | Val Acc: 0.734768 loss: 0.842480\n",
            "saving model with acc 0.735\n",
            "[013/200] Train Acc: 0.693458 Loss: 1.025725 | Val Acc: 0.738012 loss: 0.829250\n",
            "saving model with acc 0.738\n",
            "[014/200] Train Acc: 0.696775 Loss: 1.006814 | Val Acc: 0.739906 loss: 0.814803\n",
            "saving model with acc 0.740\n",
            "[015/200] Train Acc: 0.700495 Loss: 0.990196 | Val Acc: 0.741646 loss: 0.806322\n",
            "saving model with acc 0.742\n",
            "[016/200] Train Acc: 0.702585 Loss: 0.976805 | Val Acc: 0.743947 loss: 0.796677\n",
            "saving model with acc 0.744\n",
            "[017/200] Train Acc: 0.706467 Loss: 0.960459 | Val Acc: 0.745524 loss: 0.787406\n",
            "saving model with acc 0.746\n",
            "[018/200] Train Acc: 0.709016 Loss: 0.948058 | Val Acc: 0.747793 loss: 0.779635\n",
            "saving model with acc 0.748\n",
            "[019/200] Train Acc: 0.712412 Loss: 0.934735 | Val Acc: 0.748118 loss: 0.771767\n",
            "saving model with acc 0.748\n",
            "[020/200] Train Acc: 0.714413 Loss: 0.923861 | Val Acc: 0.749817 loss: 0.766786\n",
            "saving model with acc 0.750\n",
            "[021/200] Train Acc: 0.717149 Loss: 0.913509 | Val Acc: 0.750923 loss: 0.758800\n",
            "saving model with acc 0.751\n",
            "[022/200] Train Acc: 0.719299 Loss: 0.903800 | Val Acc: 0.752931 loss: 0.754867\n",
            "saving model with acc 0.753\n",
            "[023/200] Train Acc: 0.721651 Loss: 0.894471 | Val Acc: 0.753671 loss: 0.750697\n",
            "saving model with acc 0.754\n",
            "[024/200] Train Acc: 0.723669 Loss: 0.885951 | Val Acc: 0.755134 loss: 0.744718\n",
            "saving model with acc 0.755\n",
            "[025/200] Train Acc: 0.725487 Loss: 0.876950 | Val Acc: 0.755419 loss: 0.742833\n",
            "saving model with acc 0.755\n",
            "[026/200] Train Acc: 0.727991 Loss: 0.867390 | Val Acc: 0.757167 loss: 0.736182\n",
            "saving model with acc 0.757\n",
            "[027/200] Train Acc: 0.729535 Loss: 0.861110 | Val Acc: 0.756004 loss: 0.736793\n",
            "[028/200] Train Acc: 0.731541 Loss: 0.853358 | Val Acc: 0.758257 loss: 0.730639\n",
            "saving model with acc 0.758\n",
            "[029/200] Train Acc: 0.733308 Loss: 0.845552 | Val Acc: 0.759403 loss: 0.727678\n",
            "saving model with acc 0.759\n",
            "[030/200] Train Acc: 0.735642 Loss: 0.837369 | Val Acc: 0.759208 loss: 0.725252\n",
            "[031/200] Train Acc: 0.737184 Loss: 0.832084 | Val Acc: 0.759566 loss: 0.722938\n",
            "saving model with acc 0.760\n",
            "[032/200] Train Acc: 0.738524 Loss: 0.824429 | Val Acc: 0.761192 loss: 0.719035\n",
            "saving model with acc 0.761\n",
            "[033/200] Train Acc: 0.739630 Loss: 0.818789 | Val Acc: 0.761639 loss: 0.717096\n",
            "saving model with acc 0.762\n",
            "[034/200] Train Acc: 0.741910 Loss: 0.811699 | Val Acc: 0.762988 loss: 0.714369\n",
            "saving model with acc 0.763\n",
            "[035/200] Train Acc: 0.742874 Loss: 0.806318 | Val Acc: 0.763801 loss: 0.713928\n",
            "saving model with acc 0.764\n",
            "[036/200] Train Acc: 0.744625 Loss: 0.799712 | Val Acc: 0.764232 loss: 0.711484\n",
            "saving model with acc 0.764\n",
            "[037/200] Train Acc: 0.746004 Loss: 0.794493 | Val Acc: 0.764590 loss: 0.710098\n",
            "saving model with acc 0.765\n",
            "[038/200] Train Acc: 0.747587 Loss: 0.789327 | Val Acc: 0.764712 loss: 0.707519\n",
            "saving model with acc 0.765\n",
            "[039/200] Train Acc: 0.748891 Loss: 0.783808 | Val Acc: 0.765663 loss: 0.705714\n",
            "saving model with acc 0.766\n",
            "[040/200] Train Acc: 0.750305 Loss: 0.777282 | Val Acc: 0.766135 loss: 0.703022\n",
            "saving model with acc 0.766\n",
            "[041/200] Train Acc: 0.751800 Loss: 0.772626 | Val Acc: 0.765306 loss: 0.705692\n",
            "[042/200] Train Acc: 0.752801 Loss: 0.767880 | Val Acc: 0.767777 loss: 0.700190\n",
            "saving model with acc 0.768\n",
            "[043/200] Train Acc: 0.754040 Loss: 0.763066 | Val Acc: 0.767745 loss: 0.700325\n",
            "[044/200] Train Acc: 0.754822 Loss: 0.758350 | Val Acc: 0.769038 loss: 0.696672\n",
            "saving model with acc 0.769\n",
            "[045/200] Train Acc: 0.755595 Loss: 0.754150 | Val Acc: 0.767810 loss: 0.698318\n",
            "[046/200] Train Acc: 0.757322 Loss: 0.749852 | Val Acc: 0.768810 loss: 0.699169\n",
            "[047/200] Train Acc: 0.758786 Loss: 0.744659 | Val Acc: 0.768062 loss: 0.696600\n",
            "[048/200] Train Acc: 0.759923 Loss: 0.741120 | Val Acc: 0.769168 loss: 0.695880\n",
            "saving model with acc 0.769\n",
            "[049/200] Train Acc: 0.761180 Loss: 0.735385 | Val Acc: 0.769948 loss: 0.693351\n",
            "saving model with acc 0.770\n",
            "[050/200] Train Acc: 0.761900 Loss: 0.732511 | Val Acc: 0.769639 loss: 0.693835\n",
            "[051/200] Train Acc: 0.763250 Loss: 0.728326 | Val Acc: 0.769623 loss: 0.693800\n",
            "[052/200] Train Acc: 0.763971 Loss: 0.724721 | Val Acc: 0.769111 loss: 0.693856\n",
            "[053/200] Train Acc: 0.764819 Loss: 0.720833 | Val Acc: 0.770225 loss: 0.690899\n",
            "saving model with acc 0.770\n",
            "[054/200] Train Acc: 0.765633 Loss: 0.717716 | Val Acc: 0.770777 loss: 0.691440\n",
            "saving model with acc 0.771\n",
            "[055/200] Train Acc: 0.766718 Loss: 0.713620 | Val Acc: 0.770753 loss: 0.693560\n",
            "[056/200] Train Acc: 0.768347 Loss: 0.709788 | Val Acc: 0.771599 loss: 0.688876\n",
            "saving model with acc 0.772\n",
            "[057/200] Train Acc: 0.769119 Loss: 0.705397 | Val Acc: 0.771761 loss: 0.688690\n",
            "saving model with acc 0.772\n",
            "[058/200] Train Acc: 0.769854 Loss: 0.702607 | Val Acc: 0.770192 loss: 0.692201\n",
            "[059/200] Train Acc: 0.770446 Loss: 0.699603 | Val Acc: 0.771135 loss: 0.690987\n",
            "[060/200] Train Acc: 0.771589 Loss: 0.696105 | Val Acc: 0.772444 loss: 0.690198\n",
            "saving model with acc 0.772\n",
            "[061/200] Train Acc: 0.771682 Loss: 0.693982 | Val Acc: 0.772859 loss: 0.688936\n",
            "saving model with acc 0.773\n",
            "[062/200] Train Acc: 0.772968 Loss: 0.691150 | Val Acc: 0.772355 loss: 0.690788\n",
            "[063/200] Train Acc: 0.774127 Loss: 0.686417 | Val Acc: 0.772176 loss: 0.691809\n",
            "[064/200] Train Acc: 0.775068 Loss: 0.684914 | Val Acc: 0.772192 loss: 0.690989\n",
            "[065/200] Train Acc: 0.775907 Loss: 0.681813 | Val Acc: 0.772786 loss: 0.689679\n",
            "[066/200] Train Acc: 0.776540 Loss: 0.676888 | Val Acc: 0.774184 loss: 0.687152\n",
            "saving model with acc 0.774\n",
            "[067/200] Train Acc: 0.777214 Loss: 0.674663 | Val Acc: 0.773192 loss: 0.690200\n",
            "[068/200] Train Acc: 0.778100 Loss: 0.672149 | Val Acc: 0.772322 loss: 0.690410\n",
            "[069/200] Train Acc: 0.779415 Loss: 0.669369 | Val Acc: 0.772599 loss: 0.691587\n",
            "[070/200] Train Acc: 0.779756 Loss: 0.666344 | Val Acc: 0.772558 loss: 0.689943\n",
            "[071/200] Train Acc: 0.780206 Loss: 0.664619 | Val Acc: 0.773265 loss: 0.689443\n",
            "[072/200] Train Acc: 0.781006 Loss: 0.662095 | Val Acc: 0.773631 loss: 0.688254\n",
            "[073/200] Train Acc: 0.781750 Loss: 0.659116 | Val Acc: 0.773778 loss: 0.689055\n",
            "[074/200] Train Acc: 0.782226 Loss: 0.657679 | Val Acc: 0.774542 loss: 0.688313\n",
            "saving model with acc 0.775\n",
            "[075/200] Train Acc: 0.783126 Loss: 0.654706 | Val Acc: 0.774200 loss: 0.690236\n",
            "[076/200] Train Acc: 0.783770 Loss: 0.651652 | Val Acc: 0.773843 loss: 0.693277\n",
            "[077/200] Train Acc: 0.784476 Loss: 0.649441 | Val Acc: 0.773713 loss: 0.691493\n",
            "[078/200] Train Acc: 0.784618 Loss: 0.648006 | Val Acc: 0.774737 loss: 0.687180\n",
            "saving model with acc 0.775\n",
            "[079/200] Train Acc: 0.785422 Loss: 0.645496 | Val Acc: 0.775347 loss: 0.687596\n",
            "saving model with acc 0.775\n",
            "[080/200] Train Acc: 0.786000 Loss: 0.643731 | Val Acc: 0.775412 loss: 0.689492\n",
            "saving model with acc 0.775\n",
            "[081/200] Train Acc: 0.786303 Loss: 0.641838 | Val Acc: 0.774509 loss: 0.690887\n",
            "[082/200] Train Acc: 0.787243 Loss: 0.639014 | Val Acc: 0.774704 loss: 0.691535\n",
            "[083/200] Train Acc: 0.787815 Loss: 0.636940 | Val Acc: 0.775217 loss: 0.688068\n",
            "[084/200] Train Acc: 0.788520 Loss: 0.636178 | Val Acc: 0.774412 loss: 0.689120\n",
            "[085/200] Train Acc: 0.788921 Loss: 0.632928 | Val Acc: 0.774517 loss: 0.691311\n",
            "[086/200] Train Acc: 0.788745 Loss: 0.631194 | Val Acc: 0.775290 loss: 0.692381\n",
            "[087/200] Train Acc: 0.789811 Loss: 0.628837 | Val Acc: 0.775802 loss: 0.687963\n",
            "saving model with acc 0.776\n",
            "[088/200] Train Acc: 0.790427 Loss: 0.627758 | Val Acc: 0.775331 loss: 0.689864\n",
            "[089/200] Train Acc: 0.790940 Loss: 0.625450 | Val Acc: 0.776282 loss: 0.688954\n",
            "saving model with acc 0.776\n",
            "[090/200] Train Acc: 0.790437 Loss: 0.625309 | Val Acc: 0.775363 loss: 0.691036\n",
            "[091/200] Train Acc: 0.791940 Loss: 0.621290 | Val Acc: 0.775452 loss: 0.690366\n",
            "[092/200] Train Acc: 0.792607 Loss: 0.619991 | Val Acc: 0.775282 loss: 0.691724\n",
            "[093/200] Train Acc: 0.792740 Loss: 0.617586 | Val Acc: 0.774818 loss: 0.694017\n",
            "[094/200] Train Acc: 0.793413 Loss: 0.617359 | Val Acc: 0.774542 loss: 0.694094\n",
            "[095/200] Train Acc: 0.793580 Loss: 0.615495 | Val Acc: 0.774680 loss: 0.693105\n",
            "[096/200] Train Acc: 0.794364 Loss: 0.613910 | Val Acc: 0.776127 loss: 0.692119\n",
            "[097/200] Train Acc: 0.793931 Loss: 0.612647 | Val Acc: 0.775436 loss: 0.691645\n",
            "[098/200] Train Acc: 0.795014 Loss: 0.611365 | Val Acc: 0.775168 loss: 0.692970\n",
            "[099/200] Train Acc: 0.795959 Loss: 0.607668 | Val Acc: 0.774729 loss: 0.697865\n",
            "[100/200] Train Acc: 0.795996 Loss: 0.607870 | Val Acc: 0.775103 loss: 0.694541\n",
            "[101/200] Train Acc: 0.796196 Loss: 0.605990 | Val Acc: 0.775005 loss: 0.694916\n",
            "[102/200] Train Acc: 0.797017 Loss: 0.604278 | Val Acc: 0.774566 loss: 0.696621\n",
            "[103/200] Train Acc: 0.797347 Loss: 0.601724 | Val Acc: 0.775631 loss: 0.695016\n",
            "[104/200] Train Acc: 0.797922 Loss: 0.600579 | Val Acc: 0.774786 loss: 0.696268\n",
            "[105/200] Train Acc: 0.798423 Loss: 0.599800 | Val Acc: 0.774973 loss: 0.696714\n",
            "[106/200] Train Acc: 0.798387 Loss: 0.598270 | Val Acc: 0.775412 loss: 0.695983\n",
            "[107/200] Train Acc: 0.798464 Loss: 0.597843 | Val Acc: 0.776005 loss: 0.694025\n",
            "[108/200] Train Acc: 0.799819 Loss: 0.595026 | Val Acc: 0.775745 loss: 0.696126\n",
            "[109/200] Train Acc: 0.799515 Loss: 0.594573 | Val Acc: 0.774680 loss: 0.698514\n",
            "[110/200] Train Acc: 0.800160 Loss: 0.592479 | Val Acc: 0.776347 loss: 0.694677\n",
            "saving model with acc 0.776\n",
            "[111/200] Train Acc: 0.800927 Loss: 0.591019 | Val Acc: 0.774501 loss: 0.695612\n",
            "[112/200] Train Acc: 0.800643 Loss: 0.590298 | Val Acc: 0.774753 loss: 0.697639\n",
            "[113/200] Train Acc: 0.800941 Loss: 0.589851 | Val Acc: 0.774680 loss: 0.699144\n",
            "[114/200] Train Acc: 0.801942 Loss: 0.587461 | Val Acc: 0.774087 loss: 0.696383\n",
            "[115/200] Train Acc: 0.801314 Loss: 0.586952 | Val Acc: 0.775314 loss: 0.697382\n",
            "[116/200] Train Acc: 0.801832 Loss: 0.586558 | Val Acc: 0.775745 loss: 0.696536\n",
            "[117/200] Train Acc: 0.802243 Loss: 0.584256 | Val Acc: 0.775078 loss: 0.697677\n",
            "[118/200] Train Acc: 0.803190 Loss: 0.582356 | Val Acc: 0.774428 loss: 0.700190\n",
            "[119/200] Train Acc: 0.803045 Loss: 0.582246 | Val Acc: 0.775818 loss: 0.697286\n",
            "[120/200] Train Acc: 0.803839 Loss: 0.580731 | Val Acc: 0.776371 loss: 0.699707\n",
            "saving model with acc 0.776\n",
            "[121/200] Train Acc: 0.803721 Loss: 0.580885 | Val Acc: 0.775965 loss: 0.698510\n",
            "[122/200] Train Acc: 0.804028 Loss: 0.579232 | Val Acc: 0.775997 loss: 0.699127\n",
            "[123/200] Train Acc: 0.804583 Loss: 0.577752 | Val Acc: 0.775452 loss: 0.703034\n",
            "[124/200] Train Acc: 0.804383 Loss: 0.577297 | Val Acc: 0.776331 loss: 0.698600\n",
            "[125/200] Train Acc: 0.805281 Loss: 0.575314 | Val Acc: 0.775826 loss: 0.698618\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-0061d5934d56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set the model to training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hi7jTn3PX-m"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfUECMFCn5VG"
      },
      "source": [
        "Create a testing dataset, and load model from the saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PKjtAScPWtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9037db18-46cc-42c8-8577-d501f864c3f2"
      },
      "source": [
        "# create testing dataset\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# create model and load weights from checkpoint\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "940TtCCdoYd0"
      },
      "source": [
        "Make prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84HU5GGjPqR0"
      },
      "source": [
        "predict = []\n",
        "model.eval() # set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWDf_C-omElb"
      },
      "source": [
        "Write prediction to a CSV file.\n",
        "\n",
        "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuljYSPHcZir"
      },
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}